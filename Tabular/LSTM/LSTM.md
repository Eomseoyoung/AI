# 1. LSTM이란
  LSTM은 **순환 신경망(RNN)**의 한 종류이며, 특히 시퀀스 데이터(Sequence Data)를 처리하는 데 뛰어난 성능을 보임.
  RNN의 한계 극복: 기본적인 RNN은 시퀀스의 길이가 길어질수록 초기에 입력된 정보가 소실되는 기울기 소실(Vanishing Gradient) 문제를 겪습니다. 이로 인해 과거의 중요한 정보를 기억하지 못하는 장기 의존성(Long-Term Dependencies) 문제가 발생.

  해결책: LSTM은 이 문제를 해결하기 위해 **메모리 셀(Memory Cell)**과 게이트(Gate) 메커니즘을 도입하여, 정보의 흐름을 능동적으로 제어하고 장기간의 정보를 효과적으로 기억하거나 잊을 수 있도록 설계되었음.

  주요 응용 분야: 자연어 처리(기계 번역, 감성 분석), 음성 인식, 시계열 데이터 예측 등 순서가 중요한 모든 분야에서 널리 활용.


# 🧠 Long Short-Term Memory (LSTM) 모델 요약


> 1. LSTM의 등장 배경 및 역할정의 : LSTM은 순환 신경망(RNN)의 한 종류입니다.

> 필요성: 기존 RNN은 시퀀스가 길어질 때 기울기 소실(Vanishing Gradient) 문제로 인해 과거의 중요한 정보를 잊어버리는 장기 의존성(Long-Term Dependencies) 문제가 있었습니다.
> 역할: LSTM은 게이트(Gate) 메커니즘을 통해 정보의 흐름을 제어함으로써, 이 문제를 해결하고 긴 시퀀스에서도 정보를 효율적으로 기억하고 유지할 수 있도록 설계되었습니다.
> 활용: 자연어 처리, 음성 인식, 시계열 예측 등 순서와 문맥이 중요한 분야에 필수적입니다.


> 2. LSTM의 핵심 메커니즘: 셀 스테이트와 게이트LSTM의 혁신은 **셀 스테이트(Cell State)**라는 장기 기억 경로와 이를 제어하는 세 가지 게이트에 있습니다.

>     1. 셀 스테이트 (Cell State, $C_t$)기능: 정보가 시퀀스를 따라 거의 변형 없이 흐르는 "컨베이어 벨트" 역할을 합니다. 이를 통해 정보가 오랫동안 보존됩니다.
>     2. 세 가지 제어 게이트정보의 유입, 유지, 출력을 조절하는 3개의 게이트는 시그모이드 함수($\sigma$)를 통해 0과 1 사이의 값으로 정보의 통과량을 결정합니다.
>     3. 게이트 이름,역할,간단 설명

> <img width="692" height="200" alt="image" src="https://github.com/user-attachments/assets/0947142b-7578-48fe-87db-c588a00ac9ba" />

>     게이트 이름역할간단 설명망각 게이트 ($f_t$)과거 정보 제거이전 셀 스테이트($C_{t-1}$)에서 불필요한 정보를 얼마나 잊을지 결정합니다.

>     입력 게이트 ($i_t$)새 정보 추가현재 입력($x_t$)에서 어떤 새로운 정보를 셀 스테이트에 저장할지 결정합니다.

>     출력 게이트 ($o_t$)최종 은닉 상태 결정최종 업데이트된 셀 스테이트($C_t$)를 기반으로 다음 시점의 출력($h_t$)을 생성합니다.


> 3. 📝 요약LSTM은 복잡한 게이트 시스템을 통해 정보를 선택적으로 기억하고 잊는 능력을 모델에 부여하여, 시퀀스 데이터 분석의 정확도와 성능을 비약적으로 향상시킨 핵심 딥러닝 구조
>
> 4.  단점
>    정보 압축: 셀 상태($C_t$)는 시퀀스의 긴 과거 정보를 압축하여 담고 있지만, 매 단계마다 모든 과거 정보가 하나의 벡터로 압축되어 전달됩니다.

>    시퀀스가 매우 길어지면, 셀 상태가 가장 중요한 초기 정보를 '잊어버리는' 경향이 나타날 수 있습니다.
> 
>    Transformer와의 비교: 최근에는 어텐션 메커니즘을 사용하는 Transformer 모델이 시퀀스의 거리에 관계없이 모든 위치를 한 번에 참조할 수 있기 때문에, LSTM보다 장기 의존성
>    포착에 더 강력함을 보입니다.

