> ### 4. 양방향 분할 네트워크 (Bilateral Segmentation Network)
> 
> BiSeNet의 개념은 특정 구현에 종속되지 않는 범용적인 구조로, 다양한 합성곱 모델과 설계에 적용될 수 있다.
> 
> 본 연구의 핵심은 다음 세 가지이다.
> 1. Detail Branch는 작은 수용 영역과 얕은 층을 사용하여 공간적 디테일을 담당한다.
> 2. Semantic Branch는큰 수용 영역과 깊은 층을 사용하여 범주적 의미 정보를 담당한다.
> 3. 두 종류의 특징 표현을 효과적으로 결합하기 위한 효율적인 Aggregation Layer를 설계한다.
>
> 본 절에서는 Figure 3에 제시된 전체 아키텍처와 각 구성 요소의 구체적인 구현을 설명한다.

> ### 4.1 Detail Branch
>
> Table 1에 제시된 Detail Branch는 총 3개의 스테이지로 구성된다.
>
> 각 스테이지의 모든 계층은 다음 순서로 이루어진다.
> * 합성곱(Convolution)
> * 배치 정규화(Batch Normalization)
> * 활성화 함수(ReLU)
>
> 각 스테이지의 첫 번째 계층은 stride = 2를 사용하며,
> 같은 스테이지 내의 나머지 계층들은 동일한 필터 수와 출력 해상도를 유지한다.
>
> 그 결과 Detail Branch의 출력 특징 맵 해상도는 입력 대비 1/8이 된다.
>
> 이 분기는 높은 채널 수를 사용하기 때문에 풍부한 공간 디테일을 인코딩할 수 있다.
> 그러나 채널 수와 공간 해상도가 모두 크기 때문에, 잔차구조(residual structure)는 메모리 접근 비용을 증가시킨다.
>
> 따라서 본 연구에서는 ResNet 계열이 아닌,
> VGG 스타일의 단순한 계층 스택 구조를 채택한다.

> ### 4.2 Semantic Branch
> Semantic Branch는 큰 수용 영역 확보와 연산 효율성을 동시에 만족해야 한다.
> 이를 위해 MobileNet, Xception, ShuffleNet 등 경량 인식 모델의 설계 철학을 차용한다.
>
> Semantic Branch의 주요 구성 요소는 다음과 같다.

> ### 4.2.1 Stem Block
> Stem Block은 Semantic Branch의 첫 번째 단계로 사용된다.
> 
> 이 블록은 두 가지 서로 다른 다운샘플링 경로를 사용한다.
> * 3x3 Convolution (stride = 2)
> * 3x3 Max Pooling (stride = 2)
> 이 두 경로의 출력을 채널 방향으로 결합하여 최종 출력을 생성한다.
>
> 이 구조는:
> * 계산 비용이 낮고
> * 특징 표현력이 뛰어나다

> ### 4.2.2 Context Embedding Block
> Semantic Branch는 고수준 의미 정보를 포착하기 위해 큰 수용 영역이 필요하다.
> 이를 위해 Context Embedding Block를 설계한다.
>
> 이 블록은 다음으로 구성된다.
> 1. Global Average Pooling을 통해 전역 문맥 정보 추출
> 2. 1×1 Convolution을 통해 채널 변환
> 3. 잔차 연결(residual connection)을 통해 원래 특징과 결합
>
> 이를 통해 전역 문맥 정보를 효율적으로 임베딩할 수 있다.

> ### 4.2.3 Gather-and-Expansion (GE) Layer
> 깊이 분리 합성곱(depth-wise convolution)의 장점을 활용하여,
> 본 연구에서는 Gather-and-Expansion Layer를 제안한다.
>
> GE Layer의 구성은 다음과 같다.
> 1. 3×3 Convolution (Gather)
>    -> 지역 특징을 수립하고 고차원 공간으로 확장
> 2. 3×3 Depth-wise Convolution
>    -> 각 채널을 독립적으로 처리
> 3. 1×1 Convolution (Projection)
>    -> 저차원 채널 공간으로 투영
>    
> stride = 2인 경우에는:
> 
> * 두 개의 3×3 depth-wise convolution을 사용하여 수용 영역을 더욱 확대
> * shortcut 경로로 3×3 separable convolution 사용
>   
> 기존 MobileNetV2의 inverted bottleneck과 비교하면:
> 
> * GE Layer는 3×3 Convolution이 하나 더 추가되었음
> * 하지만 CUDA 라이브러리에서 3×3 Convolution은 고도로 최적화되어 있어 실제 연산 비용은 여전히 효율적이다.
>   
> 또한 GE Layer는 inverted boottleneck보다 더 높은 표현력을 제공한다.

> ### 4.3 Bilateral Guided Aggregation Layer
> 
> Detail Branch와 Semantic Branch의 출력은 서로 다른 수준의 특징 표현을 가진다.
>
> * Detail Branch -> 저수준 공간 정보
> * Semantic Branch -> 고수준 의미 정보
>
> 단순 합이나 결합은 이러한 차이를 무시하게 되며, 이는 성능 저하와 학습 불안정으로 이어진다.
>
> ### 제안: Bilateral Guided Aggregation (BGA)
> 
> 본 연구에서는 Semantic Branch의 문맥 정보를 가이드로 사용하여 Detail Branch의 특징을 조정하는 Bilateral Guided Aggregation Layer를 제안한다
>
> 이 방식의 장점은 다음과 같다.
>
> * Semantic 정보가 Detail 정보를 다중 스케일로 가이드
> * 서로 다른 해상도의 특징을 효과적으로 결합
> * 단순 결합보다 더 효율적이고 성능이 우수
>
> 이 집계 방식은 본질적으로 멀티스케일 정보를 암묵적으로 인코딩한다.
>
> 
