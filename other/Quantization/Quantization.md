> # Quantization(양자화)
> -----------------------------------------------
> ### 개념
>   * 모델의 가중치, 활성값을 저정밀도(int8, int4 등)로 변환
>   * 연산량, 메모리 사용 감소
>
>     * FP32(기본) → FP16(반정밀) → INT8(양자화) → INT4
>    
> ### 대표 방식
> 
> #### 1. Post_training Quantization (PTQ)
>      
>    * 학습 후 바로 양자화
>    * 데이터 소량으로 calibration
>    * 장점 : 빠름, 학습 재실행 불필요
>    * 단점 : 정확도 하락 가능성 큼
>
>      
> #### 2. Quantization-Aware Training (QAT)
>
>   * 학습 중 양자화 오차를 고려
>   * 장점 
>     * 정확도 손실 최소화
>     *  INT8 실전 배포에 가장 안정적
>       
>   * 단점
>     *  학습 비용 증가
>     *  구현 복잡
>
